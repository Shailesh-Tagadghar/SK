# Practical 1: Implement Feed Forward NN and train the network with different optimizers and compare


//pip install tensorflow
# Import Statements
import tensorflow as tf
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split as tts
from sklearn.preprocessing import LabelBinarizer

# Load Iris Dataset
iris = load_iris()

# Get Features and output
X = iris.data
y = iris.target

# One-hot encode labels
lb = LabelBinarizer()
y = lb.fit_transform(y)

# Split data into train and test sets
X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, random_state=42)

# Define Model Architecture
model = tf.keras.Sequential([
    tf.keras.layers.Dense(16, activation='relu', input_shape=(4,)),
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])

# Compile model with different optimizers
optimizers = ['adam', 'sgd', 'rmsprop']
for optimizer in optimizers:
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    # Train model
    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=0) # Try with different epochs and verbose



OUTPUT : 

/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)
Optimizer adam
Test loss: 0.5168842077255249
Test accuracy: 0.8333333134651184
Optimizer sgd
Test loss: 0.3008347749710083
Test accuracy: 1.0
Optimizer rmsprop
Test loss: 0.1649055927991867
Test accuracy: 1.0

    # Evaluate model
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    print('Optimizer', optimizer)
    print('Test loss:', loss)
    print('Test accuracy:', accuracy)
